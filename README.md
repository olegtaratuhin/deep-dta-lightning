<div align="center">

# DeepDTA in PyTorch-Lightning

<a href="https://pytorch.org/get-started/locally/"><img alt="PyTorch" src="https://img.shields.io/badge/PyTorch-ee4c2c?logo=pytorch&logoColor=white"></a>
<a href="https://pytorchlightning.ai/"><img alt="Lightning" src="https://img.shields.io/badge/-Lightning-792ee5?logo=pytorchlightning&logoColor=white"></a>
<a href="https://hydra.cc/"><img alt="Config: Hydra" src="https://img.shields.io/badge/Config-Hydra-89b8cd"></a>
<a href="https://github.com/ashleve/lightning-hydra-template"><img alt="Template" src="https://img.shields.io/badge/-Lightning--Hydra--Template-017F2F?style=flat&logo=github&labelColor=gray"></a><br>
\[[DeepDTA paper](https://academic.oup.com/bioinformatics/article/34/17/i821/5093245)\]
\[[DeepDTA repository](https://github.com/hkmztrk/DeepDTA)\]

</div>

<br>

## Description

Reimplementation of the original [DeepDTA](https://github.com/hkmztrk/DeepDTA) code rewritten in up-to-date Pytorch.
Default parameters are set to best recommended in the [original paper](https://academic.oup.com/bioinformatics/article/34/17/i821/5093245).

This repo allows to easily experiment with different model architectures, different datasets and different hardware.

## Installation

### Poetry

```bash
# clone project
git clone https://github.com/olegtaratuhin/deep-dta-lightning
cd deep-dta-lightning

# Install dependencies using poetry
poetry install
```

## How to run

Train model with default configuration (Davis dataset is by default)

```bash
# train on CPU
python src/train.py trainer=cpu

# train on GPU
python src/train.py trainer=gpu

# train on mac MPS (might be slower than CPU, depends on data and model sizes)
python src/train.py trainer=mps
```

To train with KIBA:

```bash
python src/train.py data=kiba
```

You can override any parameter from command line like this

```bash
python src/train.py trainer.max_epochs=20 data.batch_size=64
```

## Prepare new data

In order to add your custom dataset you need to add a new config file to `configs/<new data>.yaml`.
Follow the existing ones as examples. They use the following format:

1. `proteins.json` - contains an array of iso encoded proteins
2. `ligands.json` - contains an array of smiles encoded molecules
3. `affinity.npy` - numpy float matrix with drug-protein affinity. The indices should be as in those files above.

Alternatively, you might define a new datamodule that would output training samples in the same format.

<details>
<summary><b>Project structure</b></summary>

## Project Structure

The directory structure follows the template:

```
├── .github                   <- Github Actions workflows
│
├── configs                   <- Hydra configs
│   ├── callbacks                <- Callbacks configs
│   ├── data                     <- Data configs
│   ├── debug                    <- Debugging configs
│   ├── experiment               <- Experiment configs
│   ├── extras                   <- Extra utilities configs
│   ├── hparams_search           <- Hyperparameter search configs
│   ├── hydra                    <- Hydra configs
│   ├── local                    <- Local configs
│   ├── logger                   <- Logger configs
│   ├── model                    <- Model configs
│   ├── paths                    <- Project paths configs
│   ├── trainer                  <- Trainer configs
│   │
│   ├── eval.yaml             <- Main config for evaluation
│   └── train.yaml            <- Main config for training
│
├── data                   <- Project data
│
├── logs                   <- Logs generated by hydra and lightning loggers
│
├── notebooks              <- Jupyter notebooks. Naming convention is a number (for ordering),
│                             the creator's initials, and a short `-` delimited description,
│                             e.g. `1.0-jqp-initial-data-exploration.ipynb`.
│
├── scripts                <- Shell scripts
│
├── src                    <- Source code
│   ├── data                     <- Data scripts
│   ├── models                   <- Model scripts
│   ├── utils                    <- Utility scripts
│   │
│   ├── eval.py                  <- Run evaluation
│   └── train.py                 <- Run training
│
├── tests                  <- Tests of any kind
│
├── .env.example              <- Example of file for storing private environment variables
├── .gitignore                <- List of files ignored by git
├── .pre-commit-config.yaml   <- Configuration of pre-commit hooks for code formatting
├── .project-root             <- File for inferring the position of project root directory
├── environment.yaml          <- File for installing conda environment
├── Makefile                  <- Makefile with commands like `make train` or `make test`
├── pyproject.toml            <- Configuration options for testing and linting
├── requirements.txt          <- File for installing python dependencies
├── setup.py                  <- File for installing project as a package
└── README.md
```

</details>
